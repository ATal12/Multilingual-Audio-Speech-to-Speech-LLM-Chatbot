{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d322c617-a494-4d57-8383-7b0d46b2beab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:13:14.917220Z",
     "iopub.status.busy": "2024-08-17T20:13:14.916946Z",
     "iopub.status.idle": "2024-08-17T20:13:23.739080Z",
     "shell.execute_reply": "2024-08-17T20:13:23.738582Z",
     "shell.execute_reply.started": "2024-08-17T20:13:14.917204Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faster-whisper\n",
      "  Downloading faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting av<13,>=11.0 (from faster-whisper)\n",
      "  Downloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
      "  Downloading ctranslate2-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.20.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.15.1)\n",
      "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
      "  Downloading onnxruntime-1.19.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (69.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/lib/python3/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (23.2)\n",
      "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.23.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Downloading faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ctranslate2-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.19.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
      "Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.3.1 faster-whisper-1.0.3 humanfriendly-10.0 onnxruntime-1.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "#INSTALL WHISPERER FOR CONVERTING SPEECH TO TEXT- ANY OF 99 LANGUAGES\n",
    "!pip install faster-whisper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c16550b-3e4a-4871-8ae0-ad1a22fc9558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:13:23.740099Z",
     "iopub.status.busy": "2024-08-17T20:13:23.739922Z",
     "iopub.status.idle": "2024-08-17T20:13:28.512649Z",
     "shell.execute_reply": "2024-08-17T20:13:28.512170Z",
     "shell.execute_reply.started": "2024-08-17T20:13:23.740082Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A4000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cab53b-c1c7-49bc-b91b-53855b781017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:13:28.514920Z",
     "iopub.status.busy": "2024-08-17T20:13:28.514696Z",
     "iopub.status.idle": "2024-08-17T20:13:53.533011Z",
     "shell.execute_reply": "2024-08-17T20:13:53.532341Z",
     "shell.execute_reply.started": "2024-08-17T20:13:28.514904Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bdfe37f11849da9f7b0c03c2e9c95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61684bea502490d9cfbc4f8180328cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755d45ae7ef745d48c8a333b382296fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d5da0c09a14ca7873e4ea74cf581b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c2b45ae605431dadb5793d420cf895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 0.996582\n",
      "[0.00s -> 6.00s]  It was rather interesting just to watch them gathering their materials and bouncing around.\n",
      "[6.00s -> 7.00s]  Bouncing around.\n",
      "[7.00s -> 12.00s]  Yeah, they're what they call a kangaroo walk or something like that.\n",
      "[12.00s -> 13.00s]  Who named it that?\n",
      "[13.00s -> 14.00s]  I don't know.\n",
      "[14.00s -> 19.00s]  I bet those men are going to get quite a reception when they get back to Earth.\n",
      "[19.00s -> 20.00s]  Oh, yes.\n",
      "[20.00s -> 28.00s]  I'll be so glad when they land back now, but I think that's pretty well a fact because\n",
      "[28.00s -> 32.00s]  they've landed so many safely now that I feel relieved.\n",
      "[32.00s -> 35.00s]  Just getting off of the moon was the thing that was...\n",
      "[35.00s -> 38.00s]  Have they met with the one that was circling?\n",
      "[38.00s -> 42.00s]  Yes, they've rendezvoused, so I understand.\n",
      "[42.00s -> 49.00s]  That wasn't shown either, but they say they have rendezvoused, so that's a matter of\n",
      "[49.00s -> 53.00s]  making the circles and then coming down.\n",
      "[53.00s -> 56.00s]  What do you sort of imagine for the future?\n",
      "[56.00s -> 58.00s]  Do you imagine them standing up?\n",
      "[58.00s -> 59.00s]  I think they will.\n",
      "[59.00s -> 62.00s]  I think they will do some more exploring up there.\n",
      "[62.00s -> 65.00s]  Very positive.\n",
      "[65.00s -> 71.00s]  Because that was such a very small area, when you think of it, that they just gathered\n",
      "[71.00s -> 82.00s]  rocks and samples of soil and all, and they did probe for some samples.\n",
      "[82.00s -> 85.00s]  Just what's going to come of that, I don't know.\n",
      "[85.00s -> 86.00s]  I'll be glad to read it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TEST WHISPER ON A 1969 INTERVIEW AFTER THE MOON LANDING - DOWNLOADED\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# INSTALL MODEL LARGE V3\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "#segments, info = model.transcribe(\"audio.mp3\", beam_size=5)\n",
    "segments, info = model.transcribe(\"CA138clip.mp3\", beam_size=5)\n",
    "\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bde3d05-2a80-4fbf-aec2-b318c5a6abea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T21:26:13.724081Z",
     "iopub.status.busy": "2024-08-17T21:26:13.723494Z",
     "iopub.status.idle": "2024-08-17T21:26:18.329635Z",
     "shell.execute_reply": "2024-08-17T21:26:18.328561Z",
     "shell.execute_reply.started": "2024-08-17T21:26:13.724059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'ar' with probability 0.999023\n",
      "[0.00s -> 2.64s]  سأزور دبي لمدة ثلاث أيام\n",
      "[2.64s -> 7.78s]  يرجى اقتراح أماكن يمكن رؤيتها خلال زياراتي لدبي\n",
      " سأزور دبي لمدة ثلاث أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زياراتي لدبي.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# INSTALL MODEL LARGE V3\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8\")\n",
    "\n",
    "\n",
    "#MALE PERSON ASKING THE FOLLOWING QUESTION IN ARABIC\n",
    "# #I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
    "##I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
    "#inputtext = \"\"\"سأزور دبي لمدة 3 أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زيارتي لدبي.\"\"\"\n",
    "segments2, info2 = model.transcribe(\"male-arabic-mp3-dubaivisitquestion3days.mp3\", beam_size=5)\n",
    "\n",
    "##Spanish text\n",
    "#I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
    "#Voy a visitar Dubái durante 3 días. ¿Podrías sugerirme lugares para ver durante mi visita a Dubái?\n",
    "#Voy a visitar Dubái durante 3 días. ¿Podrías sugerirme lugares para ver durante mi visita a Dubái?\n",
    "#segments2, info2 = model.transcribe(\"male-spanish-mp3-dubaivisitquestion3days.mp3\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info2.language, info2.language_probability))\n",
    "\n",
    "segmentarabicall = \"\"\n",
    "for segment in segments2:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "    segmentarabicall = segmentarabicall + segment.text + \".\"\n",
    "    \n",
    "# NOTE THAT THE WHISPER MODEL DOES A GREAT JOB OF UNDERSTANDING THERE ARE 2 SENTENCES IN QUESTION \n",
    "#AND SPOTTING THE PAUSE    \n",
    "print(segmentarabicall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e69e47-1c3f-4456-9284-bf6c6c8b27c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:14:13.404365Z",
     "iopub.status.busy": "2024-08-17T20:14:13.404068Z",
     "iopub.status.idle": "2024-08-17T20:14:13.408137Z",
     "shell.execute_reply": "2024-08-17T20:14:13.407405Z",
     "shell.execute_reply.started": "2024-08-17T20:14:13.404345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5047d9-1d00-4cc9-99fb-ea4657399ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:32:06.925919Z",
     "iopub.status.busy": "2024-08-17T20:32:06.925683Z",
     "iopub.status.idle": "2024-08-17T20:33:02.114637Z",
     "shell.execute_reply": "2024-08-17T20:33:02.113881Z",
     "shell.execute_reply.started": "2024-08-17T20:32:06.925901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.88.tar.gz (63.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m203.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m352.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m378.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m283.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.88-cp311-cp311-linux_x86_64.whl size=3351612 sha256=346e14d45855a37a2e752b1fa8a5acaba4bf0df98ee5a75896a9dc1440101b90\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2pd4r1x6/wheels/cc/28/23/8f4df7a7f381c4ae9acb3a336b5b1bc86ab34f21e3567e16f7\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.88\n",
      "    Uninstalling llama_cpp_python-0.2.88:\n",
      "      Successfully uninstalled llama_cpp_python-0.2.88\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.1 which is incompatible.\n",
      "scipy 1.11.2 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.1 which is incompatible.\n",
      "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.1 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "matplotlib 3.7.3 requires numpy<2,>=1.20, but you have numpy 2.0.1 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.1 which is incompatible.\n",
      "pandas 2.2.0 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.0.1 which is incompatible.\n",
      "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.1 which is incompatible.\n",
      "pyarrow 15.0.0 requires numpy<2,>=1.16.6, but you have numpy 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.88 numpy-2.0.1 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# llamacpp python install for reading in llama3.1 8B Q5 MODEL\n",
    "# THIS SHOULD WORK FOR A GPU INSTALL. BUT LOOKS LIKE CPU ONLY INSTALL IN THE PAPERSPACE CLOUD - WORKS WELL BUT SLOW\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir  \\\n",
    "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "\n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.5 -DCUDAToolkit_ROOT=/usr/local/cuda-12.5 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.5/lib64\" FORCE_CMAKE=1 pip install llama-cpp-python - no-cache-dir\n",
    "\n",
    "#!CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2866bb4d-d330-477e-a3d8-c34252c17a4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:15:35.386701Z",
     "iopub.status.busy": "2024-08-17T20:15:35.386522Z",
     "iopub.status.idle": "2024-08-17T20:15:35.389724Z",
     "shell.execute_reply": "2024-08-17T20:15:35.389273Z",
     "shell.execute_reply.started": "2024-08-17T20:15:35.386684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n",
    "\n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "# THIS INSTALL COMMAND WITHOUT THE PREBUILT WHEEL STOPS INSTALL AFTER 5 MIN\n",
    "#!CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8925332a-a256-4f66-be19-4f08982a8fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:33:13.214237Z",
     "iopub.status.busy": "2024-08-17T20:33:13.213496Z",
     "iopub.status.idle": "2024-08-17T20:33:13.334670Z",
     "shell.execute_reply": "2024-08-17T20:33:13.334291Z",
     "shell.execute_reply.started": "2024-08-17T20:33:13.214212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOW DOWNLOADING LLAMA 3.1 8B GGUF MODEL FROM HUGGINGFACE\n",
    "from huggingface_hub import hf_hub_download\n",
    "import joblib\n",
    "\n",
    "REPO_ID = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\" #\"YOUR_REPO_ID\"\n",
    "FILENAME = \"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\"\n",
    "\n",
    "\n",
    "downloaded_model_path = hf_hub_download(repo_id=REPO_ID,filename=FILENAME) #, use_auth_token=True)\n",
    "print(downloaded_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "271e1262-cf20-43e3-a663-e25666e647d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:33:17.264156Z",
     "iopub.status.busy": "2024-08-17T20:33:17.262454Z",
     "iopub.status.idle": "2024-08-17T20:33:18.063335Z",
     "shell.execute_reply": "2024-08-17T20:33:18.062686Z",
     "shell.execute_reply.started": "2024-08-17T20:33:17.264132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# LOADING LLAMA 3.1 8B GGUF USING LLAMACPP PYTHON\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=downloaded_model_path,\n",
    "       n_gpu_layers=60, #-1, # Uncomment to use GPU acceleration\n",
    "       seed=1337, # Uncomment to set a specific seed\n",
    "       n_ctx=2048, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abfe482-b68b-41e3-ba83-b6c7bac1d674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:34:02.495106Z",
     "iopub.status.busy": "2024-08-17T20:34:02.494788Z",
     "iopub.status.idle": "2024-08-17T20:36:46.804072Z",
     "shell.execute_reply": "2024-08-17T20:36:46.803429Z",
     "shell.execute_reply.started": "2024-08-17T20:34:02.495088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1130: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =      73.02 ms /   757 runs   (    0.10 ms per token, 10367.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3470.31 ms /    41 tokens (   84.64 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:        eval time =  159547.63 ms /   756 runs   (  211.04 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =  164301.05 ms /   797 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-5dd92ed4-d421-47aa-a059-03d5acde4f48', 'object': 'text_completion', 'created': 1723926842, 'model': '/root/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf', 'choices': [{'text': \"\\n\\nThe City of Light! Paris, the capital of France, is a must-visit destination for any traveler. Here's a suggested 3-day itinerary for you to make the most of your trip:\\n\\n**Day 1: Classic Parisian Landmarks**\\n\\n* 9:00 AM: Start your day at the iconic Eiffel Tower (Tour Eiffel). Take the stairs or elevator to the top for breathtaking views of the city.\\n* 11:00 AM: Visit the nearby Champ de Mars park, where you can enjoy street performers, food vendors, and stunning views of the Eiffel Tower.\\n* 12:30 PM: Head to the Musée d'Orsay (Museum of Impressionism), which houses an impressive collection of Impressionist and Post-Impressionist art.\\n* 2:00 PM: Take a break for lunch at a charming bistro, such as Le Comptoir du Relais or Le Grand Vefour.\\n* 3:30 PM: Visit the nearby Louvre Museum, one of the world's largest and most famous museums, housing the Mona Lisa and other iconic works of art.\\n* 6:00 PM: End the day with a Seine River cruise, which will give you a unique perspective on the city's landmarks and bridges.\\n\\n**Day 2: Montmartre and the Latin Quarter**\\n\\n* 9:00 AM: Start the day in the bohemian neighborhood of Montmartre, famous for its narrow streets, charming cafes, and stunning views of the city. Visit the Basilique du Sacré-Cœur and explore the winding streets.\\n* 11:30 AM: Head to the nearby Place du Tertre, where you can browse the local artists' stalls and enjoy a coffee or snack.\\n* 1:00 PM: Visit the Musée de Montmartre, which showcases the history of the neighborhood and its famous artists.\\n* 2:30 PM: Explore the Latin Quarter, known for its lively atmosphere, street performers, and historic buildings. Visit the Luxembourg Gardens and the Pantheon.\\n* 6:00 PM: Enjoy dinner at a traditional French restaurant, such as Le Pied de Cochon or Le Relais de l'Entrecôte.\\n\\n**Day 3: Royal Palaces and Gardens**\\n\\n* 9:00 AM: Visit the Palace of Versailles (Château de Versailles), a former royal residence with opulent decorations and stunning gardens. Be sure to book tickets in advance to avoid long wait times.\\n* 12:30 PM: Take a break for lunch at a charming bistro near the palace.\\n* 2:00 PM: Visit the Hall of Mirrors and the Royal Chapel.\\n* 4:00 PM: Return to Paris and visit the beautiful Tuileries Garden, a peaceful oasis in the heart of the city.\\n* 6:00 PM: End your trip with a visit to the iconic Arc de Triomphe, a monument dedicated to the soldiers who fought and died for France.\\n\\nAdditional tips:\\n\\n* Consider purchasing a Paris Museum Pass, which grants you entry to many of the city's top museums and attractions.\\n* Be sure to try some of the city's famous street food, such as crepes, croissants, and escargots.\\n* Take a stroll along the Seine River and explore the city's charming neighborhoods, such as Le Marais and Belleville.\\n* Don't forget to try a French café au lait or a glass of wine at a charming café.\\n\\nThis itinerary provides a great balance of iconic landmarks, cultural experiences, and relaxation time to enjoy the beauty of Paris. Bon voyage et à bientôt à Paris!\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 756, 'total_tokens': 797}}\n",
      "\n",
      "\n",
      "The City of Light! Paris, the capital of France, is a must-visit destination for any traveler. Here's a suggested 3-day itinerary for you to make the most of your trip:\n",
      "\n",
      "**Day 1: Classic Parisian Landmarks**\n",
      "\n",
      "* 9:00 AM: Start your day at the iconic Eiffel Tower (Tour Eiffel). Take the stairs or elevator to the top for breathtaking views of the city.\n",
      "* 11:00 AM: Visit the nearby Champ de Mars park, where you can enjoy street performers, food vendors, and stunning views of the Eiffel Tower.\n",
      "* 12:30 PM: Head to the Musée d'Orsay (Museum of Impressionism), which houses an impressive collection of Impressionist and Post-Impressionist art.\n",
      "* 2:00 PM: Take a break for lunch at a charming bistro, such as Le Comptoir du Relais or Le Grand Vefour.\n",
      "* 3:30 PM: Visit the nearby Louvre Museum, one of the world's largest and most famous museums, housing the Mona Lisa and other iconic works of art.\n",
      "* 6:00 PM: End the day with a Seine River cruise, which will give you a unique perspective on the city's landmarks and bridges.\n",
      "\n",
      "**Day 2: Montmartre and the Latin Quarter**\n",
      "\n",
      "* 9:00 AM: Start the day in the bohemian neighborhood of Montmartre, famous for its narrow streets, charming cafes, and stunning views of the city. Visit the Basilique du Sacré-Cœur and explore the winding streets.\n",
      "* 11:30 AM: Head to the nearby Place du Tertre, where you can browse the local artists' stalls and enjoy a coffee or snack.\n",
      "* 1:00 PM: Visit the Musée de Montmartre, which showcases the history of the neighborhood and its famous artists.\n",
      "* 2:30 PM: Explore the Latin Quarter, known for its lively atmosphere, street performers, and historic buildings. Visit the Luxembourg Gardens and the Pantheon.\n",
      "* 6:00 PM: Enjoy dinner at a traditional French restaurant, such as Le Pied de Cochon or Le Relais de l'Entrecôte.\n",
      "\n",
      "**Day 3: Royal Palaces and Gardens**\n",
      "\n",
      "* 9:00 AM: Visit the Palace of Versailles (Château de Versailles), a former royal residence with opulent decorations and stunning gardens. Be sure to book tickets in advance to avoid long wait times.\n",
      "* 12:30 PM: Take a break for lunch at a charming bistro near the palace.\n",
      "* 2:00 PM: Visit the Hall of Mirrors and the Royal Chapel.\n",
      "* 4:00 PM: Return to Paris and visit the beautiful Tuileries Garden, a peaceful oasis in the heart of the city.\n",
      "* 6:00 PM: End your trip with a visit to the iconic Arc de Triomphe, a monument dedicated to the soldiers who fought and died for France.\n",
      "\n",
      "Additional tips:\n",
      "\n",
      "* Consider purchasing a Paris Museum Pass, which grants you entry to many of the city's top museums and attractions.\n",
      "* Be sure to try some of the city's famous street food, such as crepes, croissants, and escargots.\n",
      "* Take a stroll along the Seine River and explore the city's charming neighborhoods, such as Le Marais and Belleville.\n",
      "* Don't forget to try a French café au lait or a glass of wine at a charming café.\n",
      "\n",
      "This itinerary provides a great balance of iconic landmarks, cultural experiences, and relaxation time to enjoy the beauty of Paris. Bon voyage et à bientôt à Paris!\n"
     ]
    }
   ],
   "source": [
    "# DO A RANDOM TEST OF THE LLM HERE TO ENSURE ITS WORKING AS EXPECTED\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Provide an itinerary for places to see in Paris over a 3-day trip<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "\n",
    "output = llm(\n",
    "      prompt, # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)\n",
    "\n",
    "#out2=(output['choices'])\n",
    "\n",
    "#out2=(output['choices'])\n",
    "#print(out2[0]['text'])\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf9e19-e10f-47dd-901b-a98c0a11b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd24c47d-3e5a-4c28-8bc1-d7c73d4e8ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T21:27:02.719725Z",
     "iopub.status.busy": "2024-08-17T21:27:02.719184Z",
     "iopub.status.idle": "2024-08-17T21:29:33.550788Z",
     "shell.execute_reply": "2024-08-17T21:29:33.550181Z",
     "shell.execute_reply.started": "2024-08-17T21:27:02.719705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant that answers user questions politely and respecfully in the same language as the user's question. Please answer the Question below in the same language as the user's question.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Question:  سأزور دبي لمدة ثلاث أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زياراتي لدبي.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1130: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "Llama.generate: 55 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =      65.38 ms /   619 runs   (    0.11 ms per token,  9467.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2572.77 ms /    37 tokens (   69.53 ms per token,    14.38 tokens per second)\n",
      "llama_print_timings:        eval time =  127285.55 ms /   618 runs   (  205.96 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:       total time =  130837.07 ms /   655 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "سوف أكون سعيدًا بالتأكيد في مساعدتك في تخطيط رحلتك إلى دبي. دبي هي مدينة رائعة مليئة بالمعالم السياحية والأنشطة المتعددة. إليك بعض المقترحات الرائعة لزيارتك:\n",
      "\n",
      "1. **بورج خليفة**: برج خليفة هو أطول برج في العالم، وواحد من أبرز المعالم السياحية في دبي. يمكنك الاستمتاع بمنظر المدينة المذهل من أعلى البرج.\n",
      "2. **مركز دبي التجاري العالمي**: يحتوي المركز على عدد من المباني الهامة، مثل برج العرب، وهو معلم سياحي يحتوي على مطاعم وفنادق وكذلك متاجر تسوق فاخرة.\n",
      "3. **طواحين القمح القديمة**: هؤلاء الطواحين القديمة هي من بين أقدم النماذج المحفوظة من الطواحين القديمة في الإمارات العربية المتحدة، تعرض للزيارة للاستمتاع بالتاريخ والثقافة المحلية.\n",
      "4. **دار الحكمة**: دار الحكمة هي متحف يستعرض تاريخ الإمارات وتاريخ دبي، وهي مكان رائع للتعرف على الحضارة والثقافة المحلية.\n",
      "5. **منتزهات دبي**: دبي تضم العديد من المنتزهات الجميلة، مثل منتزهات ديرة وفلورا هيلس وبرج النخيل. يمكنك الاستمتاع بالجو الخلاب وتنزهاتك في هذه المنتزهات.\n",
      "6. **سوق دبي للوثائق السريعة**: هذا السوق هو مكان رائع للاستمتاع بالتجارة والاستكشاف لمنتجاتها الغنية.\n",
      "7. **متنزهات كيست**: منطقة كيست في دبي تعتبر وجهة رائعة للرياضة والملاهي والاسترخاء.\n",
      "8. **فندق البطريق**: هذا الفندق هو أطول مبنى في دبي، يمكنك استكشاف المزيد عن هذا المبنى الفخم.\n",
      "9. **متنزه الميناء**: هذا المتنزه يضم مجموعة واسعة من الأنشطة والترفيه، بما في ذلك جبال التزلج والقوارب والرحلات البحرية.\n",
      "10. **متحف الفن الحديث في دبي**: هذا المتحف هو أحد أهم المعالم الثقافية في دبي، يضم مجموعة واسعة من الأعمال الفنية الرائعة.\n",
      "\n",
      "هذه بعض المقترحات لزيارتك لضمان تجربة ممتعة ومثمرة في دبي. لا تنسَ إستكشاف الأماكن المحلية والفنادق الرائعة والأنشطة التي ستقدمها لك المدينة.\n",
      "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\n",
      "سوف أكون سعيدًا بالتأكيد في مساعدتك في تخطيط رحلتك إلى دبي. دبي هي مدينة رائعة مليئة بالمعالم السياحية والأنشطة المتعددة. إليك بعض المقترحات الرائعة لزيارتك:\n",
      "\n",
      "1. **بورج خليفة**: برج خليفة هو أطول برج في العالم، وواحد من أبرز المعالم السياحية في دبي. يمكنك الاستمتاع بمنظر المدينة المذهل من أعلى البرج.\n",
      "2. **مركز دبي التجاري العالمي**: يحتوي المركز على عدد من المباني الهامة، مثل برج العرب، وهو معلم سياحي يحتوي على مطاعم وفنادق وكذلك متاجر تسوق فاخرة.\n",
      "3. **طواحين القمح القديمة**: هؤلاء الطواحين القديمة هي من بين أقدم النماذج المحفوظة من الطواحين القديمة في الإمارات العربية المتحدة، تعرض للزيارة للاستمتاع بالتاريخ والثقافة المحلية.\n",
      "4. **دار الحكمة**: دار الحكمة هي متحف يستعرض تاريخ الإمارات وتاريخ دبي، وهي مكان رائع للتعرف على الحضارة والثقافة المحلية.\n",
      "5. **منتزهات دبي**: دبي تضم العديد من المنتزهات الجميلة، مثل منتزهات ديرة وفلورا هيلس وبرج النخيل. يمكنك الاستمتاع بالجو الخلاب وتنزهاتك في هذه المنتزهات.\n",
      "6. **سوق دبي للوثائق السريعة**: هذا السوق هو مكان رائع للاستمتاع بالتجارة والاستكشاف لمنتجاتها الغنية.\n",
      "7. **متنزهات كيست**: منطقة كيست في دبي تعتبر وجهة رائعة للرياضة والملاهي والاسترخاء.\n",
      "8. **فندق البطريق**: هذا الفندق هو أطول مبنى في دبي، يمكنك استكشاف المزيد عن هذا المبنى الفخم.\n",
      "9. **متنزه الميناء**: هذا المتنزه يضم مجموعة واسعة من الأنشطة والترفيه، بما في ذلك جبال التزلج والقوارب والرحلات البحرية.\n",
      "10. **متحف الفن الحديث في دبي**: هذا المتحف هو أحد أهم المعالم الثقافية في دبي، يضم مجموعة واسعة من الأعمال الفنية الرائعة.\n",
      "\n",
      "هذه بعض المقترحات لزيارتك لضمان تجربة ممتعة ومثمرة في دبي. لا تنسَ إستكشاف الأماكن المحلية والفنادق الرائعة والأنشطة التي ستقدمها لك المدينة.\n"
     ]
    }
   ],
   "source": [
    "## **** THIS PART OF THE TEXT MAINTAINS THE CONVERSATIONS AND LLM RESPONSES IN THE SAME NATIVE LANGUAGE\n",
    "# OF THE SPEAKER/USER THROUGHOUT THE CONVERSATIONAL CHAIN\n",
    "inputtextarabic = segmentarabicall \n",
    "\n",
    "promptrespondnativelanguage = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant that answers user questions politely and respecfully in the same language as the user's question. Please answer the Question below in the same language as the user's question.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question: {inputtextarabic}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "print(promptrespondnativelanguage)\n",
    "\n",
    "outputrespondnativelanguage = llm(\n",
    "      promptrespondnativelanguage, # Prompt\n",
    "      max_tokens=800, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "#print(output)\n",
    "\n",
    "#print(type(output))\n",
    "#out2=(output['choices'])\n",
    "#print(type(out2)) #['text'])\n",
    "#print(out2[0]['text'])\n",
    "\n",
    "llmresponsenativelanguage = outputrespondnativelanguage['choices'][0]['text']\n",
    "print(llmresponsenativelanguage)\n",
    "\n",
    "!pip install gTTS\n",
    "\n",
    "## SAVE THE LLM RESPONSE INTO AN MP3 AUDIO FILE\n",
    "print(llmresponsenativelanguage)\n",
    "from gtts import gTTS\n",
    "tts = gTTS(llmresponsenativelanguage,lang='ar')\n",
    "#tts.save('llmspokenresponse_arabic_fromarabicquestion_noxlate.mp3')\n",
    "\n",
    "tts.save('llmspokenresponse_spanish_fromspanishquestion_noxlate.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a31ddf-6229-4c03-9339-55208c2974c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:38:31.925897Z",
     "iopub.status.busy": "2024-08-17T20:38:31.925737Z",
     "iopub.status.idle": "2024-08-17T20:40:05.464387Z",
     "shell.execute_reply": "2024-08-17T20:40:05.463818Z",
     "shell.execute_reply.started": "2024-08-17T20:38:31.925887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "بالتأكيد، سوف أضع ليك قائمة بأهم الأماكن السياحية في دبي التي يمكنك زيارتها خلال زيارتك:\n",
      "\n",
      "1.  برج خليفة: وهو أطول مبنى في العالم وواحد من المعالم الرئيسية في دبي. يمكنك رؤية منظرًا بانوراميًا لمنطقة دبي من أعلى البرج.\n",
      "2.  مرسى دبي: وهو مرفأ صغير يقع في الشارقة، لكنه يعتبر من المعالم السياحية الشهيرة في الإمارات العربية المتحدة. يمكنك رؤية السفن والحوادث القديمة هناك.\n",
      "3.  دبي مول: وهو واحد من أكبر مراكز التسوق في العالم، يمكنك شراء المنتجات الفاخرة أو الاستمتاع بالعديد من الأنشطة هناك.\n",
      "4.  جزيرة النخيل: هي جزيرة ساحلية منظرية في دبي، وهي وجهة مثالية للراحة والاستجمام.\n",
      "5.  مطار دبي الدولي: وهو واحد من أشهر مطارات العالم، يمكنك رؤية طائرات من جميع أنواعها هناك.\n",
      "6.  متحف دبي للفنون: هو متحف للفنون يركز على الفن الإسلامي، يمكنك رؤية أعمال فنية متنوعة هناك.\n",
      "7.  دبي لاند: هو منتجع سياحي يتضمن حدائق ومرافق رياضية، يمكنك الاستمتاع بالعديد من الأنشطة هناك.\n",
      "8.  كواليتي جيتز دبي: هو منتجع سياحي يضم ملاهي ومرافق رياضية، يمكنك الاستمتاع بالعديد من الأنشطة هناك.\n",
      "\n",
      "سوف أكون سعيدًا إن ساعدتك هذه المعلومات في خطتك لزيارة دبي.\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant that accurately translates arabic text to english. Translate only the Input text below to English without any additional output<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Input: \n",
      "\n",
      "بالتأكيد، سوف أضع ليك قائمة بأهم الأماكن السياحية في دبي التي يمكنك زيارتها خلال زيارتك:\n",
      "\n",
      "1.  برج خليفة: وهو أطول مبنى في العالم وواحد من المعالم الرئيسية في دبي. يمكنك رؤية منظرًا بانوراميًا لمنطقة دبي من أعلى البرج.\n",
      "2.  مرسى دبي: وهو مرفأ صغير يقع في الشارقة، لكنه يعتبر من المعالم السياحية الشهيرة في الإمارات العربية المتحدة. يمكنك رؤية السفن والحوادث القديمة هناك.\n",
      "3.  دبي مول: وهو واحد من أكبر مراكز التسوق في العالم، يمكنك شراء المنتجات الفاخرة أو الاستمتاع بالعديد من الأنشطة هناك.\n",
      "4.  جزيرة النخيل: هي جزيرة ساحلية منظرية في دبي، وهي وجهة مثالية للراحة والاستجمام.\n",
      "5.  مطار دبي الدولي: وهو واحد من أشهر مطارات العالم، يمكنك رؤية طائرات من جميع أنواعها هناك.\n",
      "6.  متحف دبي للفنون: هو متحف للفنون يركز على الفن الإسلامي، يمكنك رؤية أعمال فنية متنوعة هناك.\n",
      "7.  دبي لاند: هو منتجع سياحي يتضمن حدائق ومرافق رياضية، يمكنك الاستمتاع بالعديد من الأنشطة هناك.\n",
      "8.  كواليتي جيتز دبي: هو منتجع سياحي يضم ملاهي ومرافق رياضية، يمكنك الاستمتاع بالعديد من الأنشطة هناك.\n",
      "\n",
      "سوف أكون سعيدًا إن ساعدتك هذه المعلومات في خطتك لزيارة دبي.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1130: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "Llama.generate: 13 prefix-match hit, remaining 435 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =      27.82 ms /   294 runs   (    0.09 ms per token, 10568.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31269.44 ms /   435 tokens (   71.88 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:        eval time =   61879.03 ms /   293 runs   (  211.19 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   93528.34 ms /   728 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sure! Here is the translation:\n",
      "\n",
      "Of course, I'll give you a list of the most important tourist attractions in Dubai to visit during your visit:\n",
      "\n",
      "1.  Burj Khalifa: It is the tallest building in the world and one of Dubai's main landmarks. You can see a panoramic view of the Dubai area from the top of the tower.\n",
      "2.  Dubai Marina: It is a small harbor located in Sharjah, but it is considered one of the most famous tourist attractions in the United Arab Emirates. You can see ships and old boats there.\n",
      "3.  Dubai Mall: It is one of the largest shopping centers in the world, you can buy luxury products or enjoy many activities there.\n",
      "4.  Palm Jumeirah: It is a coastal island in Dubai, a perfect destination for relaxation and recreation.\n",
      "5.  Dubai International Airport: It is one of the most famous airports in the world, you can see airplanes of all kinds there.\n",
      "6.  Dubai Museum of Art: It is an art museum that focuses on Islamic art, you can see various works of art there.\n",
      "7.  Dubai Land: It is a tourist resort that includes gardens and sports facilities, you can enjoy many activities there.\n",
      "8.  Quality Jumeirah Dubai: It is a tourist resort that includes entertainment and sports facilities, you can enjoy many activities there.\n",
      "\n",
      "I will be glad if this information helps you plan your trip to Dubai.\n"
     ]
    }
   ],
   "source": [
    "## JUST FOR INTERNAL VALIDATION, TRANSLATE THE CONVERSATION INTO ENGLISH\n",
    "print(llmresponsenativelanguage)\n",
    "prompttranslateartoen = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant that accurately translates arabic text to english. Translate only the Input text below to English without any additional output<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Input: {llmresponsenativelanguage}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "print(prompttranslateartoen)\n",
    "\n",
    "outputllmresponse_artoen = llm(\n",
    "      prompttranslateartoen, # Prompt\n",
    "      max_tokens=800, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "#print(output)\n",
    "\n",
    "#print(type(output))\n",
    "#out2=(output['choices'])\n",
    "#print(type(out2)) #['text'])\n",
    "#print(out2[0]['text'])\n",
    "\n",
    "llmresponse_artoen = outputllmresponse_artoen['choices'][0]['text']\n",
    "print(llmresponse_artoen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a973f032-83e4-4724-abb7-b3cc496283c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:44:14.327039Z",
     "iopub.status.busy": "2024-08-17T20:44:14.326529Z",
     "iopub.status.idle": "2024-08-17T20:44:24.945576Z",
     "shell.execute_reply": "2024-08-17T20:44:24.945162Z",
     "shell.execute_reply.started": "2024-08-17T20:44:14.327020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant that accurately translates arabic text to english. Translate only the Input text below to English without any additional output<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Input:  سأزور دبي لمدة ثلاث أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زياراتي لدبي.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1130: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "Llama.generate: 15 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =       2.88 ms /    21 runs   (    0.14 ms per token,  7291.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6486.37 ms /    64 tokens (  101.35 ms per token,     9.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4102.58 ms /    20 runs   (  205.13 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =   10611.13 ms /    84 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I will visit Dubai for three days. Please suggest places to visit during my trip to Dubai.\n"
     ]
    }
   ],
   "source": [
    "## THE REST OF THIS CODE IS REALLY A BACKUP FOR INTERNAL VALIDATION AND EVALUATION OF EACH STEP OF THE\n",
    "# WORKFLOW IN ENGLISH - THIS IS NEEDED SINCE I DONT UNDERSTAND FOREGN LANGUAGES LIKE ARABIC\n",
    "#**** THIS PART OF THE CODE TRANSLATES ALL QUESTIONS AND CONVERSATIONS TO ENGLISH, AND THEN BACKTRANSLATES \n",
    "#***  THE FINAL AI RESPONSES BACK TO ARABIC\n",
    "#print(segment.text)\n",
    "inputtext = segmentarabicall #segment.text\n",
    "\n",
    "# INITIAL TESTS WITH MANUALLY TYPED QUESTIONS\n",
    "# tell me which places I can visit in Dubai for a 3-day trip\n",
    "#inputtext = \"\"\"أخبرني عن الأماكن التي يمكنني زيارتها في دبي في رحلة لمدة 3 أيام\"\"\"\n",
    "\n",
    "\n",
    "#I am visiting Dubai for 3 days. Please suggest an itinerary for my visit.\n",
    "#inputtext = \"\"\"سأزور دبي لمدة 3 أيام. يرجى اقتراح خط سير لزيارتي.\"\"\"\n",
    "\n",
    "# THIS IS MANUALLY TYPED TEXT - NOT AUDIO\n",
    "#I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
    "#inputtext = \"\"\"سأزور دبي لمدة 3 أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زيارتي لدبي.\"\"\"\n",
    "\n",
    "inputtext = segmentarabicall \n",
    "\n",
    "\n",
    "\n",
    "prompttranslate = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant that accurately translates arabic text to english. Translate only the Input text below to English without any additional output<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Input: {inputtext}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "print(prompttranslate)\n",
    "\n",
    "output = llm(\n",
    "      prompttranslate, # Prompt\n",
    "      max_tokens=256, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "#print(output)\n",
    "\n",
    "\n",
    "translatedtext = output['choices'][0]['text']\n",
    "print(translatedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ced5c167-b9bd-48ab-bcef-387e122d58e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:44:42.513256Z",
     "iopub.status.busy": "2024-08-17T20:44:42.512800Z",
     "iopub.status.idle": "2024-08-17T20:46:33.518704Z",
     "shell.execute_reply": "2024-08-17T20:46:33.518260Z",
     "shell.execute_reply.started": "2024-08-17T20:44:42.513236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant that answers user questions politely and respecfully. Please answer the Question below.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Question: \n",
      "\n",
      "I will visit Dubai for three days. Please suggest places to visit during my trip to Dubai.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 13 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =      47.02 ms /   512 runs   (    0.09 ms per token, 10890.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5231.91 ms /    48 tokens (  109.00 ms per token,     9.17 tokens per second)\n",
      "llama_print_timings:        eval time =  105032.39 ms /   511 runs   (  205.54 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =  110995.34 ms /   559 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You're planning a trip to Dubai! I'd be delighted to help you with that.\n",
      "\n",
      "Dubai is a vibrant city that offers a mix of traditional Arabic culture, modern architecture, and world-class shopping. Here are some must-visit places to add to your itinerary for your three-day trip:\n",
      "\n",
      "**Day 1:**\n",
      "\n",
      "1. **Burj Khalifa**: Start your day by visiting the tallest building in the world, Burj Khalifa. Take the high-speed elevator to the observation deck on the 124th floor for breathtaking views of the city.\n",
      "2. **Dubai Mall**: After visiting Burj Khalifa, head to Dubai Mall, one of the largest shopping centers in the world. You can shop, dine, or catch a movie at the mall's cinema complex.\n",
      "3. **Dubai Fountain Show**: Catch the Dubai Fountain Show, a spectacular display of water, music, and light that's a must-see experience.\n",
      "\n",
      "**Day 2:**\n",
      "\n",
      "1. **Palm Jumeirah**: Take a monorail ride to Palm Jumeirah, a man-made island in the shape of a palm tree. Enjoy the stunning views of the Arabian Gulf and the city skyline.\n",
      "2. **Atlantis, The Palm**: Visit the luxurious Atlantis, The Palm resort, and explore its beautiful beaches, water parks, and marine habitats.\n",
      "3. **Jumeirah Mosque**: Visit the Jumeirah Mosque, a stunning example of modern Islamic architecture, and take a guided tour to learn about its significance.\n",
      "\n",
      "**Day 3:**\n",
      "\n",
      "1. **Desert Safari**: Take a thrilling desert safari adventure, which includes dune bashing, camel riding, and a visit to a Bedouin camp for a traditional Arabic lunch.\n",
      "2. **Global Village**: Visit the Global Village, a cultural and entertainment park that showcases the traditions and customs of different countries from around the world.\n",
      "3. **Dubai Miracle Garden**: End your trip with a visit to the Dubai Miracle Garden, a floral wonderland with over 45 million flowers arranged in stunning displays.\n",
      "\n",
      "Additional tips:\n",
      "\n",
      "* Consider purchasing a Dubai Tourist Pass, which grants you access to many attractions and experiences at a discounted rate.\n",
      "* Don't forget to try some local cuisine, such as shawarma, falafel, or Arabic coffee.\n",
      "* Be prepared for the heat and sun by staying hydrated and taking breaks in shaded areas.\n",
      "\n",
      "Enjoy your trip to Dubai, and I hope this helps you make the most of your three-day itinerary!\n"
     ]
    }
   ],
   "source": [
    "promptrespond = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant that answers user questions politely and respecfully. Please answer the Question below.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question: {translatedtext}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "print(promptrespond)\n",
    "\n",
    "outputrespond = llm(\n",
    "      promptrespond, # Prompt\n",
    "      max_tokens=800, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "#print(output)\n",
    "\n",
    "#print(type(output))\n",
    "#out2=(output['choices'])\n",
    "#print(type(out2)) #['text'])\n",
    "#print(out2[0]['text'])\n",
    "\n",
    "llmresponse = outputrespond['choices'][0]['text']\n",
    "print(llmresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a62198-f868-49cc-b9de-869377cf2e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:46:48.848073Z",
     "iopub.status.busy": "2024-08-17T20:46:48.847763Z",
     "iopub.status.idle": "2024-08-17T20:47:06.102398Z",
     "shell.execute_reply": "2024-08-17T20:47:06.101764Z",
     "shell.execute_reply.started": "2024-08-17T20:46:48.848049Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gTTS\n",
    "\n",
    "from gtts import gTTS\n",
    "tts = gTTS(llmresponse)\n",
    "tts.save('llmresponse-english.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "472acb13-056f-406c-826a-695ca1ef0372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:47:06.103449Z",
     "iopub.status.busy": "2024-08-17T20:47:06.103258Z",
     "iopub.status.idle": "2024-08-17T20:50:01.559568Z",
     "shell.execute_reply": "2024-08-17T20:50:01.559170Z",
     "shell.execute_reply.started": "2024-08-17T20:47:06.103432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant that accurately translates English text to Arabic. Translate only the Input text below to Arabic without any additional output<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Input: \n",
      "\n",
      "You're planning a trip to Dubai! I'd be delighted to help you with that.\n",
      "\n",
      "Dubai is a vibrant city that offers a mix of traditional Arabic culture, modern architecture, and world-class shopping. Here are some must-visit places to add to your itinerary for your three-day trip:\n",
      "\n",
      "**Day 1:**\n",
      "\n",
      "1. **Burj Khalifa**: Start your day by visiting the tallest building in the world, Burj Khalifa. Take the high-speed elevator to the observation deck on the 124th floor for breathtaking views of the city.\n",
      "2. **Dubai Mall**: After visiting Burj Khalifa, head to Dubai Mall, one of the largest shopping centers in the world. You can shop, dine, or catch a movie at the mall's cinema complex.\n",
      "3. **Dubai Fountain Show**: Catch the Dubai Fountain Show, a spectacular display of water, music, and light that's a must-see experience.\n",
      "\n",
      "**Day 2:**\n",
      "\n",
      "1. **Palm Jumeirah**: Take a monorail ride to Palm Jumeirah, a man-made island in the shape of a palm tree. Enjoy the stunning views of the Arabian Gulf and the city skyline.\n",
      "2. **Atlantis, The Palm**: Visit the luxurious Atlantis, The Palm resort, and explore its beautiful beaches, water parks, and marine habitats.\n",
      "3. **Jumeirah Mosque**: Visit the Jumeirah Mosque, a stunning example of modern Islamic architecture, and take a guided tour to learn about its significance.\n",
      "\n",
      "**Day 3:**\n",
      "\n",
      "1. **Desert Safari**: Take a thrilling desert safari adventure, which includes dune bashing, camel riding, and a visit to a Bedouin camp for a traditional Arabic lunch.\n",
      "2. **Global Village**: Visit the Global Village, a cultural and entertainment park that showcases the traditions and customs of different countries from around the world.\n",
      "3. **Dubai Miracle Garden**: End your trip with a visit to the Dubai Miracle Garden, a floral wonderland with over 45 million flowers arranged in stunning displays.\n",
      "\n",
      "Additional tips:\n",
      "\n",
      "* Consider purchasing a Dubai Tourist Pass, which grants you access to many attractions and experiences at a discounted rate.\n",
      "* Don't forget to try some local cuisine, such as shawarma, falafel, or Arabic coffee.\n",
      "* Be prepared for the heat and sun by staying hydrated and taking breaks in shaded areas.\n",
      "\n",
      "Enjoy your trip to Dubai, and I hope this helps you make the most of your three-day itinerary!<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1130: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "Llama.generate: 13 prefix-match hit, remaining 543 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    3470.45 ms\n",
      "llama_print_timings:      sample time =      63.88 ms /   641 runs   (    0.10 ms per token, 10034.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41668.08 ms /   543 tokens (   76.74 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:        eval time =  132756.56 ms /   640 runs   (  207.43 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:       total time =  175448.43 ms /  1183 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "تخططين رحلة إلى دبي! سأكون سعيدًا في مساعدتك في ذلك.\n",
      "\n",
      "دبي مدينة حيوية تقدم مزيج من الثقافة العربية التقليدية والархيتكture الحديثة والمتاحف العالمية. وهذه بعض الأماكن التي يجب زيارتها لزيادة جدول رحلتك لمدة ثلاثة أيام:\n",
      "\n",
      "**يوم 1:**\n",
      "\n",
      "1. **برج خليفة**: تبدأ يومك بزيارة أعلى مبنى في العالم، برج خليفة. استخدم المصعد السريع لمتعة المشاهدة للمرصد على الطابق 124 لتجربة مشاهدة المدينة.\n",
      "2. **مॉल دبي**: بعد زيارة برج خليفة، اذهب إلى مॉल دبي، واحد من أكبر المراكز التسويقية في العالم. يمكنك التسوق أو الأكل أو رؤية فيلم في معرض السينما.\n",
      "3. **مهرجان دبي للمياه**: شاهد عرض دبي للمياه، عرض مذهل من الماء والغناء والضوء الذي يجب أن تكون تجربة.\n",
      "\n",
      "**يوم 2:**\n",
      "\n",
      "1. **رمح جوهرة**: اتخذ قطارًا سريعًا لرمح جوهرة، جزيرة مصنوعة يدويًا في شكل شجرة التمر. استمتع بالمناظر الساحرة للخليج العربي والمنظر المعماري.\n",
      "\n",
      "2. **أتلسيس، جزيرة جوهرة**: زور فندق أتلانتيز، جوهرة، وفحص المتنزهات والمتنزهات البحرية.\n",
      "\n",
      "3. **مسجد جوهرة**: زور مسجد جوهرة، مثال مذهل عن المعمار الإسلام الحديث، وخذ دورة م_GUIDED لتعلم أهمية المسجد.\n",
      "\n",
      "**يوم 3:**\n",
      "\n",
      "1. **رحلة صحراوية**: اتخذ رحلة صحراوية مثيرة، التي تشمل التصادم على الرمال والفرس النسرى والزيارة لمخيم بدوي لتناول طعام تقليدي عربي.\n",
      "\n",
      "2. **جامع العالمي**: زور الجامع العالمي، حديقة ثقافية ومتعة تشمل أساطير وطقوس مختلفة من جميع أنحاء العالم.\n",
      "\n",
      "3. **حديقة معجزة دبي**: انتهِ رحلتك بزيارة حديقة معجزة دبي، حديقة زهرية مذهلة بأكثر من 45 مليون زهرة.\n",
      "\n",
      "أفكار إضافية:\n",
      "\n",
      "* اشرح في شراء بطاقة دبي السياحية، التي تمنحك الوصول إلى العديد من المعالم والمشاعر في سعر منخفض.\n",
      "* لا تنسى تجربة الطعام المحلي، مثل شاورما أو فلافل أو قهوة عربية.\n",
      "* استعد للحرارة والمشمس بالبقاء مغسولًا والمقاومة للمناطق المظللة.\n"
     ]
    }
   ],
   "source": [
    "#print(inputtext)\n",
    "prompttranslatetoarabic = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant that accurately translates English text to Arabic. Translate only the Input text below to Arabic without any additional output<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Input: {llmresponse}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "print(prompttranslatetoarabic)\n",
    "\n",
    "llmoutputarabictextfull = llm(\n",
    "      prompttranslatetoarabic, # Prompt\n",
    "      max_tokens=800, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      #echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "#print(output)\n",
    "\n",
    "#print(type(output))\n",
    "#out2=(output['choices'])\n",
    "#print(type(out2)) #['text'])\n",
    "#print(out2[0]['text'])\n",
    "\n",
    "llmoutputarabictext = llmoutputarabictextfull['choices'][0]['text']\n",
    "print(llmoutputarabictext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a72c871-0d66-4321-a07a-b69eeb7f61dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T20:50:01.560707Z",
     "iopub.status.busy": "2024-08-17T20:50:01.560265Z",
     "iopub.status.idle": "2024-08-17T20:50:17.179986Z",
     "shell.execute_reply": "2024-08-17T20:50:17.179210Z",
     "shell.execute_reply.started": "2024-08-17T20:50:01.560691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "تخططين رحلة إلى دبي! سأكون سعيدًا في مساعدتك في ذلك.\n",
      "\n",
      "دبي مدينة حيوية تقدم مزيج من الثقافة العربية التقليدية والархيتكture الحديثة والمتاحف العالمية. وهذه بعض الأماكن التي يجب زيارتها لزيادة جدول رحلتك لمدة ثلاثة أيام:\n",
      "\n",
      "**يوم 1:**\n",
      "\n",
      "1. **برج خليفة**: تبدأ يومك بزيارة أعلى مبنى في العالم، برج خليفة. استخدم المصعد السريع لمتعة المشاهدة للمرصد على الطابق 124 لتجربة مشاهدة المدينة.\n",
      "2. **مॉल دبي**: بعد زيارة برج خليفة، اذهب إلى مॉल دبي، واحد من أكبر المراكز التسويقية في العالم. يمكنك التسوق أو الأكل أو رؤية فيلم في معرض السينما.\n",
      "3. **مهرجان دبي للمياه**: شاهد عرض دبي للمياه، عرض مذهل من الماء والغناء والضوء الذي يجب أن تكون تجربة.\n",
      "\n",
      "**يوم 2:**\n",
      "\n",
      "1. **رمح جوهرة**: اتخذ قطارًا سريعًا لرمح جوهرة، جزيرة مصنوعة يدويًا في شكل شجرة التمر. استمتع بالمناظر الساحرة للخليج العربي والمنظر المعماري.\n",
      "\n",
      "2. **أتلسيس، جزيرة جوهرة**: زور فندق أتلانتيز، جوهرة، وفحص المتنزهات والمتنزهات البحرية.\n",
      "\n",
      "3. **مسجد جوهرة**: زور مسجد جوهرة، مثال مذهل عن المعمار الإسلام الحديث، وخذ دورة م_GUIDED لتعلم أهمية المسجد.\n",
      "\n",
      "**يوم 3:**\n",
      "\n",
      "1. **رحلة صحراوية**: اتخذ رحلة صحراوية مثيرة، التي تشمل التصادم على الرمال والفرس النسرى والزيارة لمخيم بدوي لتناول طعام تقليدي عربي.\n",
      "\n",
      "2. **جامع العالمي**: زور الجامع العالمي، حديقة ثقافية ومتعة تشمل أساطير وطقوس مختلفة من جميع أنحاء العالم.\n",
      "\n",
      "3. **حديقة معجزة دبي**: انتهِ رحلتك بزيارة حديقة معجزة دبي، حديقة زهرية مذهلة بأكثر من 45 مليون زهرة.\n",
      "\n",
      "أفكار إضافية:\n",
      "\n",
      "* اشرح في شراء بطاقة دبي السياحية، التي تمنحك الوصول إلى العديد من المعالم والمشاعر في سعر منخفض.\n",
      "* لا تنسى تجربة الطعام المحلي، مثل شاورما أو فلافل أو قهوة عربية.\n",
      "* استعد للحرارة والمشمس بالبقاء مغسولًا والمقاومة للمناطق المظللة.\n"
     ]
    }
   ],
   "source": [
    "print(llmoutputarabictext)\n",
    "from gtts import gTTS\n",
    "tts = gTTS(llmoutputarabictext,lang='ar')\n",
    "tts.save('llmspokenresponse_arabic.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c279998-42c4-437f-a1ab-d405eafdf178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bc3b1-3a10-4fa6-b28a-d54df3e7ea75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
